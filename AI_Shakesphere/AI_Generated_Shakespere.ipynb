{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VcHhe2WHa_mE"
   },
   "source": [
    "# Colab Block\n",
    "I usually run my notebooks in Colabratory, which has a free 12GB GPU to connect to and use. Files are most easily accessed via a Google Drive, but that needs to be connected to and authenticated twice via this block. Also, some packages need to be installed again every runtime. So that is what this first block does, remove if not using Colab's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7gOZAXMhmJhz"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "!mkdir -p gdrive\n",
    "!google-drive-ocamlfuse gdrive\n",
    "!pip install -q keras\n",
    "!pip install numba\n",
    "!pip install tqdm\n",
    "!pip install opencv-python\n",
    "!apt update && apt install -y libsm6 libxext6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfYEK9uYbPQ0"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7762,
     "status": "ok",
     "timestamp": 1522901397989,
     "user": {
      "displayName": "Valkling FriendofAllThings",
      "photoUrl": "//lh5.googleusercontent.com/-fEy3QF-eaHI/AAAAAAAAAAI/AAAAAAAAAnM/he6PEcG-E94/s50-c-k-no/photo.jpg",
      "userId": "113638037001591500423"
     },
     "user_tz": 240
    },
    "id": "GfeDIKFRn81w",
    "outputId": "2037ff05-b690-4f87-b640-85ffac458b5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras as K\n",
    "import random\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, Embedding\n",
    "from keras.layers import Bidirectional, GRU,Flatten, Activation, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import MaxPooling1D, Conv1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import re\n",
    "import math\n",
    "# set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjz0jr--1ee8"
   },
   "source": [
    "#Read in Text\n",
    "Note: if reading in my sample weights, be sure to run every block the same until the training step. Then skip that and go right on to the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JjybK5n2mEY4"
   },
   "outputs": [],
   "source": [
    "data = open('../Shakespere_input.txt', 'r').read()\n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1522901403437,
     "user": {
      "displayName": "Valkling FriendofAllThings",
      "photoUrl": "//lh5.googleusercontent.com/-fEy3QF-eaHI/AAAAAAAAAAI/AAAAAAAAAnM/he6PEcG-E94/s50-c-k-no/photo.jpg",
      "userId": "113638037001591500423"
     },
     "user_tz": 240
    },
    "id": "iOW6aBfRNnWq",
    "outputId": "988d9d0d-f168-4cff-f786-6ebff6fc11a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "CPU times: user 21 ms, sys: 0 ns, total: 21 ms\n",
      "Wall time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "charindex = list(set(data))\n",
    "charindex.sort() \n",
    "print(charindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HEYMEw2crap"
   },
   "source": [
    "The charindex is important so keeping a backup is a good idea. This is because, either without sorting or by inputing more text files later, the index could be changed on new loads. This can result in your saved models being worthless if the charindex cannot be adequately replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qUwJFTfJ9hU0"
   },
   "outputs": [],
   "source": [
    "# np.save(\"../charindex.npy\", charindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eu7QHYS-d5Nl"
   },
   "source": [
    "# Create Sequences\n",
    "In a nutshell, the model will look at the last 75 characters in the script and attempt to predict the 76th. This block chops the .txt file into such blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54475,
     "status": "ok",
     "timestamp": 1522901458672,
     "user": {
      "displayName": "Valkling FriendofAllThings",
      "photoUrl": "//lh5.googleusercontent.com/-fEy3QF-eaHI/AAAAAAAAAAI/AAAAAAAAAnM/he6PEcG-E94/s50-c-k-no/photo.jpg",
      "userId": "113638037001591500423"
     },
     "user_tz": 240
    },
    "id": "uklgFCMzq9BU",
    "outputId": "2f748beb-93f4-4a0b-9449-776e67d9a2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.9 s, sys: 1 s, total: 53.9 s\n",
      "Wall time: 54.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CHARS_SIZE = len(charindex)\n",
    "SEQUENCE_LENGTH = 75\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(0, len(data)-SEQUENCE_LENGTH, 1 ): \n",
    "    X = data[i:i + SEQUENCE_LENGTH]\n",
    "    Y = data[i + SEQUENCE_LENGTH]\n",
    "    X_train.append([charindex.index(x) for x in X])\n",
    "    Y_train.append(charindex.index(Y))\n",
    "\n",
    "X_train = np.reshape(X_train, (len(X_train), SEQUENCE_LENGTH))\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3M-hJ4Y1uab"
   },
   "source": [
    "#Create the Model\n",
    "The model uses 3 LSTMs stacked on top of each other with 1d CNNs between.  Note that the CuDNNLSTMs are a special Nvida layer that automatically optimizes the LSTMs to work around twice as fast but needs to be used with certain GPUs. Colab's GPU is compatable and all set for it but replace with regular LSTMs if the layers won't work for you. (Still only try this code with a good GPU, this code would take too long on CPU or even an underpowerd GPU).\n",
    "\n",
    "\n",
    "Note that lighter models might work for this if computational power is an issue, though not quite as well. One LSTM/GRU with 1D CNNs can get by ok with the loopbreaker in the generation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 683,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3325,
     "status": "ok",
     "timestamp": 1522817350918,
     "user": {
      "displayName": "Valkling FriendofAllThings",
      "photoUrl": "//lh5.googleusercontent.com/-fEy3QF-eaHI/AAAAAAAAAAI/AAAAAAAAAnM/he6PEcG-E94/s50-c-k-no/photo.jpg",
      "userId": "113638037001591500423"
     },
     "user_tz": 240
    },
    "id": "KcjCh7JquaJi",
    "outputId": "dd4c0e95-0b46-42c4-9381-9a2dfb7125e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 75, 75)            2925      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_5 (CuDNNLSTM)     (None, 75, 512)           1206272   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 75, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 64)            163904    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_6 (CuDNNLSTM)     (None, 75, 512)           1183744   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 75, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 75, 128)           196736    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_7 (CuDNNLSTM)     (None, 512)               1314816   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 39)                5031      \n",
      "=================================================================\n",
      "Total params: 4,237,652\n",
      "Trainable params: 4,234,727\n",
      "Non-trainable params: 2,925\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    inp = Input(shape=(SEQUENCE_LENGTH, ))\n",
    "    x = Embedding(CHARS_SIZE, 75, trainable=False)(inp)\n",
    "    x = CuDNNLSTM(512, return_sequences=True,)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv1D(64, 5, activation=\"elu\", padding='same')(x)\n",
    "    x = CuDNNLSTM(512, return_sequences=True,)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv1D(128, 3, activation=\"elu\", padding='same')(x)\n",
    "    x = CuDNNLSTM(512,)(x)\n",
    "    x = Dense(256, activation=\"elu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation=\"elu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outp = Dense(CHARS_SIZE, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['accuracy'],\n",
    "                 )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCC19JG7117q"
   },
   "source": [
    "#Checkpoints and Custom Callback\n",
    "We will use 3 callbacks. Checkpoint, EarlyStopping, and a custom TextSample callback. Text sample prints a sample line at the end of every epoch to see how the model is progressing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bJay2oniyi4x"
   },
   "outputs": [],
   "source": [
    "filepath=\"../model_checkpoint.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                             monitor='loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"loss\",\n",
    "                      mode=\"min\",\n",
    "                      patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I_gVtu9EFPP7"
   },
   "outputs": [],
   "source": [
    "class TextSample(Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "       super(Callback, self).__init__() \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pattern = X_train[700]\n",
    "#         pattern = pattern * float(CHARS_SIZE)\n",
    "        outp = []\n",
    "        seed = [charindex[x] for x in pattern]\n",
    "        sample = 'TextSample:' +''.join(seed)+'|'\n",
    "        for t in range(100):\n",
    "          x = np.reshape(pattern, (1, len(pattern)))\n",
    "#           x = x / float(CHARS_SIZE)\n",
    "          pred = self.model.predict(x)\n",
    "          result = np.argmax(pred)\n",
    "          outp.append(result)\n",
    "          pattern = np.append(pattern,result)\n",
    "          pattern = pattern[1:len(pattern)]\n",
    "        outp = [charindex[x] for x in outp]\n",
    "        outp = ''.join(outp)\n",
    "        sample += outp\n",
    "        print(sample)\n",
    "\n",
    "textsample = TextSample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItCWuKzfidwd"
   },
   "source": [
    "# Load Model\n",
    "Load models or weights here. Github file size maximum prevents me from providing the full Shakespere model but the pretrained weights are in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11519,
     "status": "ok",
     "timestamp": 1522901472844,
     "user": {
      "displayName": "Valkling FriendofAllThings",
      "photoUrl": "//lh5.googleusercontent.com/-fEy3QF-eaHI/AAAAAAAAAAI/AAAAAAAAAnM/he6PEcG-E94/s50-c-k-no/photo.jpg",
      "userId": "113638037001591500423"
     },
     "user_tz": 240
    },
    "id": "94Csav-V5bXz",
    "outputId": "09a99e7c-22a4-4408-926f-32c07e89791c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "# model = load_model(filepath)\n",
    "# model.load_weights(\"../full_train_SP_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_922_MmgjRAO"
   },
   "source": [
    "# Train Model\n",
    "Even with Colab GPU, this can take a while. I trained the sample for ~12 hours. However, usually if gotten to roughly around 1.0 loss the generator is good enough to go. Can train almost indefinatly on most models. If the loss gets too low the text might become overfit, which in this case means just copying Shakespere in the most inefficent way possible. It should take an unrealistically long time to get to that point anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6_x9YqIK1H5k"
   },
   "outputs": [],
   "source": [
    "model_callbacks = [checkpoint, early, textsample]\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          callbacks = model_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zDGSxKKcTUb3"
   },
   "outputs": [],
   "source": [
    "# model.load_weights(filepath)\n",
    "model.save_weights(\"../full_train_weights.hdf5\")\n",
    "model.save(\"../full_train_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIPx9TSm3fpl"
   },
   "source": [
    "# Making Some New Text\n",
    "This block generates new text in the style of the input text of TEXT_LENGTH size in characters. It takes a random seed pattern from the training set, predicts the next character, adds it to the end of the pattern, then drops the first character of the pattern and predicts on the new pattern and so forth. \n",
    "## The Loopbreaker\n",
    "\n",
    "This is *very* *very* usefull technique I came up with while putting this together. Every LOOPBREAKER predictions, the program just changes one of the characters in the pattern randomly (except the last few, to prevent spelling errors). This causes our model to precieve just *slightly* different text which causes it to change it's overall predictions. Without this, even a well trained model starts to repeat itself and get caught in loops. The loopbreaker can even prevent overfitting very well or allow undertrained models to preform much better. Changing this value up and down is interesting will significantly change the output. Setting it high will have much more repeated speach, slightly lower will get many line starting the same then vering off into different dirrections, really low will get lots of varied speach but line structures and format become unstable. (It is also interesting to note that even setting most of the pattern's characters to gibberish the model can still make full words and rough lines as long as the last 10 remain untoched.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 221,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 162958,
     "status": "ok",
     "timestamp": 1522905290084,
     "user": {
      "displayName": "Valkling FriendofAllThings",
      "photoUrl": "//lh5.googleusercontent.com/-fEy3QF-eaHI/AAAAAAAAAAI/AAAAAAAAAnM/he6PEcG-E94/s50-c-k-no/photo.jpg",
      "userId": "113638037001591500423"
     },
     "user_tz": 240
    },
    "id": "uyWiDvq-2PHT",
    "outputId": "0de11620-45ea-4247-f396-bb2aeabd1cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%0.0 done\n",
      "%10.0 done\n",
      "%20.0 done\n",
      "%30.0 done\n",
      "%40.0 done\n",
      "%50.0 done\n",
      "%60.0 done\n",
      "%70.0 done\n",
      "%80.0 done\n",
      "%90.0 done\n",
      "CPU times: user 1min 45s, sys: 1min 11s, total: 2min 57s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT_LENGTH = 10000\n",
    "LOOPBREAKER = 4\n",
    "\n",
    "\n",
    "x = np.random.randint(0, len(X_train)-1)\n",
    "pattern = X_train[x]\n",
    "outp = []\n",
    "for t in range(TEXT_LENGTH):\n",
    "  if t % 100 == 0:\n",
    "    print(\"%\"+str((t/TEXT_LENGTH)*100)+\" done\")\n",
    "  \n",
    "  x = np.reshape(pattern, (1, len(pattern)))\n",
    "  pred = model.predict(x, verbose=0)\n",
    "  result = np.argmax(pred)\n",
    "  outp.append(result)\n",
    "  pattern = np.append(pattern,result)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "  ####loopbreaker####\n",
    "  if t % LOOPBREAKER == 0:\n",
    "    pattern[np.random.randint(0, len(pattern)-10)] = np.random.randint(0, len(charindex)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 6290,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1522905290561,
     "user": {
      "displayName": "Valkling FriendofAllThings",
      "photoUrl": "//lh5.googleusercontent.com/-fEy3QF-eaHI/AAAAAAAAAAI/AAAAAAAAAnM/he6PEcG-E94/s50-c-k-no/photo.jpg",
      "userId": "113638037001591500423"
     },
     "user_tz": 240
    },
    "id": "GPsyRtT5YCFT",
    "outputId": "f7a5b0c0-2bfa-46ef-eff8-c4d11f4d1e74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "at last in the streets, that he hath stain'd me.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more hear his son, and then have\n",
      "many that he hath said, and the strength of the wall,\n",
      "that have been still stabb'd and the streams of\n",
      "the strength o' the lord's part, the strongest stars\n",
      "are straited and straight\n",
      "and that the\n",
      "shearers die about the sea for a strange thing\n",
      "that i have spoke of him.\n",
      "\n",
      "menenius:\n",
      "i have said and the most marriage of the world\n",
      "that i must not be saffly as a word,\n",
      "as i have said, and the depose of the sun\n",
      "that i have spoke of honour to the world.\n",
      "\n",
      "lucio:\n",
      "i would the world but soon be so: and then i said\n",
      "the more i have attended to a wife.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast of my shame to my soul,\n",
      "and the depose they would have stay'd to hear\n",
      "her father with his hopes, and the duke\n",
      "hath been as much beloved: the subtle traitor\n",
      "the son of your string; and the sun is spotless\n",
      "that i have spoke of hostily to my soul,\n",
      "to lose his head and his\n",
      "measured to the senate, and the gods give you worse.\n",
      "\n",
      "lucio:\n",
      "i beseech you, sir, i have heard him speak at\n",
      "the strangen gallant mould of the submission,\n",
      "that have been still stabb'd in this state\n",
      "that i may see the sea from words to the sea\n",
      "where no allows have been as mine absence\n",
      "that the depose the common mouth to speak.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart to beg\n",
      "that thou hast spoke to hear the duke of york\n",
      "hath straited me to the stranger help to all the\n",
      "heaves armour that i should be as where\n",
      "i have a head as this.\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath done home all the\n",
      "duke.\n",
      "\n",
      "menenius:\n",
      "i had thought there was staying to be a king.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart of the whole child\n",
      "the white steward give the stars of war.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more than a woful suit.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart of this access\n",
      "that i may see the sea for him than he,\n",
      "and then the sun of mine hath straight at the way,\n",
      "that i may see the sea from whence thou hast\n",
      "repented to the people, that he hath\n",
      "been a submit than he hath spoke of him.\n",
      "\n",
      "menenius:\n",
      "i had thoughts are of my stay and a woman's\n",
      "master for the sheep,\n",
      "that i should say 'i hang your souls,\n",
      "and then the sun hath stronged\n",
      "that i may see the state and the depose,\n",
      "and then the sun of mine obdursing thee,\n",
      "that i may see the sea and the duke stopp'd\n",
      "than the earth would shame the words than stay.\n",
      "\n",
      "king richard iillin beggar, that have i\n",
      "with the most mistress of the state and the\n",
      "shearers and have been still the substance of\n",
      "his triumphant lords.\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath done me a man\n",
      "that should have stay'd the state and the deposing\n",
      "that thou hast spoke of his attempt of\n",
      "thing.\n",
      "\n",
      "lady capulet:\n",
      "why, then thou hast more mercy of him.\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath done my head as\n",
      "he hath a strange blood that hath a hair more\n",
      "than the sun that hath made them thank upon\n",
      "the state and the sun though\n",
      "the morning of the sun of mine own part,\n",
      "and then the sun of mine doth strike at him.\n",
      "\n",
      "king richard ii:\n",
      "what is the matter?\n",
      "\n",
      "menenius:\n",
      "i had thought the man is the fairest crown\n",
      "to him that should have stay'd the stars\n",
      "and stand him that we will be satisfied.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart that they have spoke,\n",
      "that i may see the state and their distress\n",
      "to help the word,\n",
      "that i may blow with stones that she was strength\n",
      "than he is coming to the sun of mine.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast spoke to him that would have\n",
      "a more in her own son, and then assist the sun of\n",
      "hope of the morning.\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath said to stay them.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart of this.\n",
      "\n",
      "king richard ii:\n",
      "what say you to a horse?\n",
      "\n",
      "katharina:\n",
      "i would the duke himself shows offended.\n",
      "\n",
      "menenius:\n",
      "i had thought there are married.\n",
      "\n",
      "coriolanus:\n",
      "how! what a shame speak to the purpose.\n",
      "\n",
      "lucio:\n",
      "i warrant it is the world that thou hast\n",
      "repented to the people, there\n",
      "i have a strange worse in\n",
      "the shepherd, who should be a malice.\n",
      "\n",
      "menenius:\n",
      "i had thought shall have me at all should\n",
      "be then and sent to hear them to the tower,\n",
      "that have been still supported to the fire,\n",
      "that i may see the sea and the duke\n",
      "hath been as much believed o' the state,\n",
      "that i may see many life\n",
      "and that the sight of the shepherd should be\n",
      "as any thing that he hath done already:\n",
      "the which is not the more shame of the world.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart that thou hast slander'd.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more shame to heaven that\n",
      "the manner of the shepherd give him the world.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart that would have strain'd\n",
      "the state of blood though the sun hath author\n",
      "so have i said to have her son and the\n",
      "shearence that hath stay'd the state and the deposing\n",
      "of his own lips, and then the sun is sport:\n",
      "and i have said and the process of the sun,\n",
      "how he disposed my stone before his stars.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart that thou hast slander'd\n",
      "in honour to the sun, the stars of waters,\n",
      "that have been still the worst in strong and looks\n",
      "and that the sun should be as late to see\n",
      "the state and strange and statue in the state,\n",
      "that i may blow with stones that will not have\n",
      "the state and the sun hath outwarded to the\n",
      "shepherd.\n",
      "\n",
      "menenius:\n",
      "i had thought that has more\n",
      "than the sun thoughts i have a husband looker.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more have been a subject, she hath stood the world\n",
      "that should have strain'd the shadow himself,\n",
      "that i may blow with stones, and then the world,\n",
      "that i may blow with those that would have strain'd\n",
      "the state and the sun hath outwarded to the duke.\n",
      "\n",
      "king richard in the seas.\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath done here to speak\n",
      "affect one thing that he hath stain'd.\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath done so.\n",
      "\n",
      "menenius:\n",
      "i had thought to come to the duke of york.\n",
      "\n",
      "lucio:\n",
      "i be a pitiful man that hath a strange mean\n",
      "to say 'it like a strange and as the world\n",
      "that should be so soon-hearted with the world.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart to hear you all and\n",
      "the man is lack of those that would have\n",
      "a mother shall be spent to be a man.\n",
      "\n",
      "mercutio:\n",
      "i have seen the man that is not she as\n",
      "i have at home.\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath done so.\n",
      "\n",
      "paulina:\n",
      "i have seen them in the malice and\n",
      "the state and the sun thoughts of\n",
      "the strength o' the seas\n",
      "and that the sun should be as little less\n",
      "than the earth that should not stop the world,\n",
      "that i may see the sea for the duke of york.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart that should not straight\n",
      "when i should say 'the ground is false for thee.\n",
      "\n",
      "king richard ii:\n",
      "why, then i will not stay the contrary.\n",
      "\n",
      "king richard ii:\n",
      "what say you to a good man that i say?\n",
      "\n",
      "lartius:\n",
      "the ship should she hath spoke to him already,\n",
      "and then the sun of mine doth strike\n",
      "to stand his head and that the sin hath straight\n",
      "to have her said in her acceptance.\n",
      "\n",
      "king richard ii:\n",
      "what say you to a maid?\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath done my soul of\n",
      "angelo.\n",
      "\n",
      "menenius:\n",
      "i had thought the man i saw his father'd at all.\n",
      "\n",
      "katharina:\n",
      "i have no more of this.\n",
      "\n",
      "pompey:\n",
      "i have seen a strange alteration\n",
      "that i may say 'this gentle day is speak\n",
      "to say it was the strength o' the state and the\n",
      "shepherds all that would have stay'd to have hearing\n",
      "the state and the white\n",
      "that should have stay'd the strangence have\n",
      "must be a man to them at the way for the gods\n",
      "that i may say 'silver least, although i wish,\n",
      "the morning on him\n",
      "that i may see the shadow of\n",
      "her to see him to the seas.\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath done me and the\n",
      "shearers of the shepherds of the state and\n",
      "stars in sign of the shepherd's daughter,\n",
      "and then the sun hath made him speak.\n",
      "\n",
      "lucentio:\n",
      "and the lord prince you have been the world\n",
      "that i have spoke of honour to the world.\n",
      "\n",
      "lucio:\n",
      "i warrant you, sir.\n",
      "\n",
      "menenius:\n",
      "i had thought that he would have all of you.\n",
      "\n",
      "menenius:\n",
      "i had thought that which the great distaft spirs\n",
      "with the most mighty thing that is not the world\n",
      "than thou hast spoke to hear them then alone.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart of this accident,\n",
      "that i may see\n",
      "the manner of the sun that should not speak.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart that thou hast slander'd.\n",
      "\n",
      "gloucester:\n",
      "\n",
      "king richard ii:\n",
      "why, then the master had i seen a thing\n",
      "that i may live to see him\n",
      "and the depose of the shepherd's daughter,\n",
      "thou hast more heart that thou hast spoke of wronged\n",
      "which he hath stain'd the shadow of the world,\n",
      "that i may blow with stones the state was stayed\n",
      "than the earth with thee that he would have straight\n",
      "to have her hence to see him than the world.\n",
      "\n",
      "lucio:\n",
      "i be assisted to the public parts\n",
      "which the desire hath stain'd the state and\n",
      "stark his neighbour to your honour,\n",
      "i have seen thee to the substance of the sun hath strent\n",
      "than the sun thoughts it is true time to thee\n",
      "as thou art won to be a subject good.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart of this account\n",
      "the shepherd blows from his own desires.\n",
      "\n",
      "lady capulet:\n",
      "the strangeness of your high descent,\n",
      "that have been still stabb'd with the shore\n",
      "than thou hast spoken and started four,\n",
      "and then the sun of mine own part in't speak.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart of many mouths\n",
      "at this his love and the suspicion\n",
      "of all the white hour of the state was stand\n",
      "till i have spoke of holy horse for ever.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast more heart of him.\n",
      "\n",
      "menenius:\n",
      "i had thought that he hath done me a man\n",
      "than thou hast spoke to him that hath a hair,\n",
      "and then the sun of mine hath straight\n",
      "and send the shame to her and will not stay.\n",
      "\n",
      "king richard ii:\n",
      "why, then thou hast made me be\n",
      "i thank thee.\n",
      "\n",
      "menenius:\n",
      "i have said and strange me to the\n",
      "coronation.\n",
      "\n",
      "menenius:\n",
      "i had thought that he was so, i have heard of\n",
      "coriolia.\n",
      "\n",
      "menenius:\n",
      "i had the man that hath some speed all the world\n",
      "that should be spoken; then i should not be\n",
      "constant to be a man to the other toward hours.\n",
      "\n",
      "menenius:\n",
      "i have said and strange it.\n",
      "\n",
      "lucio:\n",
      "i \n"
     ]
    }
   ],
   "source": [
    "outp = [charindex[x] for x in outp]\n",
    "outp = ''.join(outp)\n",
    "\n",
    "print(outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVfjC3Kmr6fa"
   },
   "source": [
    "# Save Text Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nBNuEBeNY9Fb"
   },
   "outputs": [],
   "source": [
    "f= open(\"../output_text.txt\",\"w\")\n",
    "f.write(outp)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Text_Generation_Model.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
